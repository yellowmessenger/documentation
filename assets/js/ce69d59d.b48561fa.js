"use strict";(self.webpackChunkbenthos=self.webpackChunkbenthos||[]).push([[72610],{28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>d});var t=s(96540);const i={},o=t.createContext(i);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(o.Provider,{value:n},e.children)}},46550:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"platform_concepts/studio/build/nodes/nodes","title":"Nodes Overview","description":"Nodes are the building blocks of a Flow/Journeys. Each node represents a specific point in the conversation where the AI-agent can perform a certain action or respond to a user\'s input.","source":"@site/docs/platform_concepts/studio/build/nodes/nodes.md","sourceDirName":"platform_concepts/studio/build/nodes","slug":"/platform_concepts/studio/build/nodes/","permalink":"/docs/platform_concepts/studio/build/nodes/","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Nodes Overview","sidebar_label":"Overview"},"sidebar":"platform_concepts","previous":{"title":"Manage flows","permalink":"/docs/platform_concepts/studio/build/Flows/manage-flows"},"next":{"title":"Dynamic chat node","permalink":"/docs/platform_concepts/studio/dynamicchatnode"}}');var i=s(74848),o=s(28453);const r={title:"Nodes Overview",sidebar_label:"Overview"},d=void 0,l={},a=[{value:"<a></a> 1. Types of nodes",id:"-1-types-of-nodes",level:2},{value:"2. Add nodes",id:"2-add-nodes",level:2},{value:"2.1 Build a flow",id:"21-build-a-flow",level:3},{value:"<a></a> 3. Node settings",id:"-3-node-settings",level:2},{value:"3.1 Configure nodes on different channels",id:"31-configure-nodes-on-different-channels",level:3},{value:"3.1 Configure node for a website",id:"31-configure-node-for-a-website",level:3},{value:"3.2 Configure node for a voice bot",id:"32-configure-node-for-a-voice-bot",level:3},{value:"Telephony related options in nodes",id:"telephony-related-options-in-nodes",level:4},{value:"Recording related options in nodes",id:"recording-related-options-in-nodes",level:4},{value:"STT related options in nodes",id:"stt-related-options-in-nodes",level:4},{value:"TTS related options in nodes",id:"tts-related-options-in-nodes",level:4},{value:"DTMF related options in nodes",id:"dtmf-related-options-in-nodes",level:4},{value:"Conversation related options in nodes",id:"conversation-related-options-in-nodes",level:4},{value:"3.3 Configure node for Google assistant",id:"33-configure-node-for-google-assistant",level:3},{value:"3.4 Configure node for Alexa",id:"34-configure-node-for-alexa",level:3},{value:"<a></a> 4. View dynamic data",id:"-4-view-dynamic-data",level:2}];function c(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Nodes are the building blocks of a ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/Flows/journeys",children:"Flow/Journeys"}),". Each node represents a specific point in the conversation where the AI-agent can perform a certain action or respond to a user's input."]}),"\n",(0,i.jsxs)(n.h2,{id:"-1-types-of-nodes",children:[(0,i.jsx)("a",{name:"types"})," 1. Types of nodes"]}),"\n",(0,i.jsx)(n.p,{children:"Nodes are classified into the following types:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Nodes"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Prompts"})}),(0,i.jsxs)(n.td,{children:["Prompts can be used when the AI-agent expects a user to respond to the posed question. Click ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/nodes/prompt-nodes",children:"here"})," to learn more."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Messages"})}),(0,i.jsxs)(n.td,{children:["Messages can be used when the AI-agent has to display information to the user without expecting any response. Click ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/nodes/message-nodes1/text-node",children:"here"})," to learn more."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Action"})}),(0,i.jsxs)(n.td,{children:["Actions are non-interactive nodes that can be used to perform a specific task. Click ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/nodes/action-nodes-overview/action-nodes",children:"here"})," to learn more."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Logic"})}),(0,i.jsxs)(n.td,{children:["Logic nodes can be used when the flow must be branched based on the given conditions. Click ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/nodes/logic-nodes",children:"here"})," to learn more."]})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["After clicking ",(0,i.jsx)(n.strong,{children:"+Create flow"}),", you will be directed to a canvas with a ",(0,i.jsx)(n.strong,{children:"start node"})," from where you will have the flexibility to design a ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/Flows/journeys",children:"flow"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/RGxw20v.png",alt:""})}),"\n",(0,i.jsx)(n.h2,{id:"2-add-nodes",children:"2. Add nodes"}),"\n",(0,i.jsx)(n.p,{children:"\xdf\nNodes can be added in two different ways:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Click the black dot in the centre of a node."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/Nn0SJLw.png",alt:""})}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsx)(n.li,{children:"Click any 4 of the icons on the left which represent each category of nodes. Drag and drop the nodes."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/olgqi1w.png",alt:""})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsx)(n.p,{children:"You cannot add more than 150 nodes in a flow."})}),"\n",(0,i.jsx)(n.h3,{id:"21-build-a-flow",children:"2.1 Build a flow"}),"\n",(0,i.jsx)(n.p,{children:"A flow is built using a series of smaller nodes. Each flow must have a minimum of two nodes configured."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Start Trigger"}),": The first node must always be a trigger (click ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/Flows/configureflow",children:"here"})," to learn more). You can configure how to trigger this flow. That is, by means of Intents, Entities, URL Events, or other flows."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Add any of the following nodes one by one to continue this flow with logic."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prompts"}),": These nodes expect user inputs- when the input is invalid, fallback messages are displayed. They are interactive/conversational nodes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Messages"}),": Use these nodes to display messages, files, images etc."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Actions"}),": Background actions like Database Insert, Search, or executing an API are performed with these nodes."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logic"}),": Conditional branching nodes."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Connect all the nodes to complete the flow and ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/tools#21-test-your-bot",children:"test"})," the AI-agent."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.h2,{id:"-3-node-settings",children:[(0,i.jsx)("a",{name:"config"})," 3. Node settings"]}),"\n",(0,i.jsxs)(n.p,{children:["Node settings can be configured to improve the use of nodes across different purposes. To access the settings, click the ",(0,i.jsx)(n.strong,{children:"tools"})," icon in each node,"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/acjZ9RG.png",alt:""})}),"\n",(0,i.jsx)(n.p,{children:"Some of the components included in Node settings are:"}),"\n",(0,i.jsx)(n.h3,{id:"31-configure-nodes-on-different-channels",children:"3.1 Configure nodes on different channels"}),"\n",(0,i.jsxs)(n.p,{children:["This option is available for all the nodes that display some information on the AI-agent. For example, message and prompt nodes like name node, text node, video node etc. It is used to configure how the information must be produced on different platforms like ",(0,i.jsx)(n.strong,{children:"Website, Alexa, Google Assistant, Voice"})," and other configured channels."]}),"\n",(0,i.jsx)(n.p,{children:"Multiple channel options are available for each type of node."}),"\n",(0,i.jsx)("img",{src:"https://i.imgur.com/xT6Xjnv.png",alt:"drawing",width:"60%"}),"\n",(0,i.jsx)(n.p,{children:"Each of these channels have multiple configurations. Take a look at them below."}),"\n",(0,i.jsx)(n.h3,{id:"31-configure-node-for-a-website",children:"3.1 Configure node for a website"}),"\n",(0,i.jsx)("img",{src:"https://i.imgur.com/7yev2mo.png",alt:"drawing",width:"60%"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hide Input"})," - When enabled, user input won't be displayed on the AI-agent."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hide Home Button"})," - When enabled and this node is executed in the flow, the user will not be able to click on the home button to restart the flow."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensitive information"})," - Can be enabled to hide passwords and other personal information from being displayed."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enable feedback"}),"- Enable this to receive feedback from your user."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Button Auto Width"})," - For Quick replies and multi select, buttons can be configured."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enable Multiple Files"})," - This is used for file-related nodes, which gives the user permission to add more than one file at once."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Enable these options to improve the video viewing experience. (available in the ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/nodes/message-nodes1/video-node",children:"Video node"})," settings)."]}),"\n",(0,i.jsx)("img",{src:"https://i.imgur.com/txLnGZL.png",alt:"drawing",width:"60%"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Thumbnail"})," - This is the image that will be displayed when the video is not being played."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Autoplay"})," - Video will play automatically."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Show controls"})," - Controls such as Play/Pause, Forward etc will be displayed."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Downloadable"})," - Displayed video can be downloaded to the local system."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Muted"})," - Mute the audio until the user unmutes it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop"})," - To play the video again once it stops."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"32-configure-node-for-a-voice-bot",children:"3.2 Configure node for a voice bot"}),"\n",(0,i.jsx)(n.p,{children:"Node-level voice options can be configured for each node specifically. The global voice options that are configured will be applicable for all the nodes and flows for the AI-agent. Whenever a global option and also node level option are defined, for that specific node, the node level option will be given more priority. For example,"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global level"}),": You can select an STT/TTS engine globally so that you don\u2019t have to configure it for each node."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Node level"}),": You can configure different \u201crecording max duration\u201d for different nodes i.e. 10 seconds for address and 5 seconds for name node."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Voice bot node options/settings are classified depending upon different uses as below:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Telephony"}),": For settings related to telephony like call forwarding, calling line identity, etc."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recording"}),": Recording options such as beep sound after a question is asked."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Speech to Text"}),": You can customize a speech recognition software that enables the recognition and translation of spoken language into text."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Text-to-Speech"}),": You can customize the Text-to-Speech (TTS) capabilities to play back text in a spoken voice."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"DTMF"}),": Dual-tone multi-frequency (DTMF) is used for touch tones, it is the sound made when pressing a number key. For cases, where we expect background noise and difficulty in correctly identifying the user utterance for numeric inputs, we can use this feature to record user responses."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Conversation"}),": Yellow cloud provides additional conversational options to further customize and elevate the experience on the IVR channel."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/CEPvxI9.png",alt:""})}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["Most of these options can be configured globally from  ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/tools#25-voice",children:"tools and settings"}),".\nIf they are configured at the node level, node level customization takes priority over the global level settings."]})}),"\n",(0,i.jsx)(n.h4,{id:"telephony-related-options-in-nodes",children:"Telephony related options in nodes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Fields"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Disconnect"})}),(0,i.jsx)(n.td,{children:"When this option is enabled, the call gets disconnected post execution of this node."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Call forward"})}),(0,i.jsx)(n.td,{children:"When this is enabled you can enter a number to forward or the SIP. It is used to enable call forwarding to an agent in any specific step.  You can either forward the call to an agent's number or forward it to some SIP extension."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Number to forward"})}),(0,i.jsx)(n.td,{children:"This textbox accepts the number fot the call to be forwarded to ex: Number (+919XXXXXXXXX)."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"SIP extension"})}),(0,i.jsx)(n.td,{children:"Extension to initiate SIP (Session Initiation Protocol) transfer."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Caller line identity"})}),(0,i.jsxs)(n.td,{children:["This field accepts ",(0,i.jsx)(n.strong,{children:"custom caller ID"})," which is sent while forwarding the call to an agent. Note, that this functionality is not supported by all the carriers."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Custom SIP header"})}),(0,i.jsx)(n.td,{children:"This can be used as an additional parameter that can be passed to an agent while transferring the call to an Agent to pass along AI-agent collected information. You can pass a key-value pair in JSON format which will get passed in the SIP header."})]})]})]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"An example of Custom SIP header:"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"[{\u201ckey\u201d:\u201cUser-to-User\u201d,  \u201cvalue\u201d:\u201cname=david&product=heater&query=not turning off&priority=high&number=12345\u201d}]"})}),"\n",(0,i.jsx)(n.h4,{id:"recording-related-options-in-nodes",children:"Recording related options in nodes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Fields"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Enable recording beep"})}),(0,i.jsx)(n.td,{children:"When this is enabled, a beep sound will be played after the AI-agent asks a question giving an auditory response to the end-user to respond."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Recording Action"})}),(0,i.jsx)(n.td,{children:"With the recording management options, you can select to pause/resume/stop recording depending upon different use-cases and conversations. By default, the recording is ON only. Also, in a call, once you STOP the recording (for recording sensitive dialogues), it can\u2019t be resumed back."})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"stt-related-options-in-nodes",children:"STT related options in nodes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Fields"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"STT engine"})}),(0,i.jsx)(n.td,{children:"Select an engine from the dropdown-  Google/Microsoft."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"STT mode"})}),(0,i.jsx)(n.td,{children:'Select mode from the dropdown. Microsoft provides "Static", "Streaming" or "Streaming Advanced". Google provides "Static".'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"STT language"})}),(0,i.jsxs)(n.td,{children:["AI-agent language(ISO code) can be selected from the dropdown. Default- English. Click ",(0,i.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/language-support?tabs=stt-tts",children:"Microsoft"})," or ",(0,i.jsx)(n.a,{href:"https://cloud.google.com/speech-to-text/docs/languages",children:"Google"})," for more information on the languages)"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"STT engine endpoint"})}),(0,i.jsx)(n.td,{children:"Endpoint id of the engine selected"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Recording max duration"})}),(0,i.jsx)(n.td,{children:"This value is the Max duration for which the AI-agent will wait after asking a question (in any step) even while the user is speaking. For example, after asking \u201cWhich city are you from?\u201d and the recording duration value is \u201c5\" - the AI-agent records only 5 seconds of user response. This option is necessary to avoid consuming unwanted information and to stay with the conversational flow. If the user mistakenly replies with long paragraphs when a question is asked or if the user's response is getting shadowed with constant background noises, the AI-agent must not process those long inputs. Hence, with this configuration, the AI-agent only takes the necessary response and can quickly process the user response."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Recording silence duration"})}),(0,i.jsx)(n.td,{children:"This value is the Max duration for which the AI-agent will wait after asking a question (in any step) for the user to respond. For example, if recording silence duration is 5 seconds, AI-agent waits for 5 seconds for the response if the user is silent. If the user does not respond anything within 6 seconds, AI-agent Message will be played."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Initial silence duration"})}),(0,i.jsx)(n.td,{children:"To provide more customization on the silence duration parameter, \u201cstreaming\u201d and \u201cstreaming-advanced\u201d STT modes (of Microsoft STT engine) allow to specifically configure the maximum acceptable silence duration before the user starts speaking.  For example, the acceptable initial silence duration for the application number question could be higher (~3/4 seconds) but in the case of a quick conversational binary question, it could be configured to 1 second."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Final silence duration"})}),(0,i.jsx)(n.td,{children:"Similar to the initial silence duration, the final silence duration is indicative of the maximum duration of pause that the AI-agent will wait for once the user has started speaking. For example, for binary/one-word questions like yes/no we could set the final silence duration to ~0.5/1.0 seconds and for address-like fields where taking a pause is intrinsic in conversation, we can set the final silence duration to ~1.5/2.5 seconds."})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"tts-related-options-in-nodes",children:"TTS related options in nodes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Fields"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"TTS engine"})}),(0,i.jsx)(n.td,{children:"Select the engines from the dropdown- Microsoft Azure, Google Wavenet, Amazon Polly."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Text type"})}),(0,i.jsx)(n.td,{children:"Select Text/SSML from the dropdown."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"TTS language"})}),(0,i.jsx)(n.td,{children:"AI-agent language(ISO code) can be selected from the dropdown."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Pitch"})}),(0,i.jsx)(n.td,{children:'Pitch value can be any decimal value depending on the base of voice required, 0 is ideal.     You can add this for Microsoft if text_type = "text" and for Google for text_type = "text" and "SSML".'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Voice ID"})}),(0,i.jsx)(n.td,{children:'Type the characters of voice ID. You can add this for Microsoft if text_type = "text" and for Google if text_type = "text" and "SSML".'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"TTS Speed"})}),(0,i.jsx)(n.td,{children:'This value defines how fast the AI-agent must converse. This value can be 0.9 - 1.5 for the AI-agent to soundly humanly.   You can add this for Microsoft if text_type = "text" and for Google if text_type = "text" and "SSML".'})]})]})]}),"\n",(0,i.jsx)(n.h4,{id:"dtmf-related-options-in-nodes",children:"DTMF related options in nodes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Fields"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Capture DTMF"})}),(0,i.jsx)(n.td,{children:"Enable this option if the DTMF is to be collected on the specific node."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Capture voice with DTMF input"})}),(0,i.jsx)(n.td,{children:"With this enabled, the AI-agent will be able to capture both voice and DTMF for the same question. Example - What is your mobile number? Note - AI-agent will only capture the one which comes first from the user be it speech response or DTMF response."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"DTMF digital length"})}),(0,i.jsx)(n.td,{children:"Enter the length of characters to be captured. Ex: For an indian phone number, it is 10."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"DTMF finish character"})}),(0,i.jsx)(n.td,{children:'Character which defines when the AI-agent must stop capturing. Supported finish characters - "*" and "#"'})]})]})]}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Either DTMF digital length or DTMF finish character can be configured.\nDTMF digit length, DTMF finish character and DTMF timeout are 3 ways in which the AI-agent understands when to stop capturing: ",(0,i.jsx)("br",{})]}),"\n"]}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Digit Length is useful when you are capturing fixed-length data. Ex: Phone number. ",(0,i.jsx)("br",{})]}),"\n",(0,i.jsxs)(n.li,{children:['Finish character is useful when you don\'t know the length and that could vary depending upon different states/products. Ex: Model ID, application number. A user can define either "*" or "#" to inform that all Digits are added. ',(0,i.jsx)("br",{})]}),"\n",(0,i.jsxs)(n.li,{children:["DTMF timeout is a default inactivity timeout (not open for configuration) and it is set to 10 seconds by default (it overrides ",(0,i.jsx)(n.strong,{children:"digit length"})," and ",(0,i.jsx)(n.strong,{children:"finish character"}),").  For example, if the length is 11 and the user has only entered 6 characters, and there are 10 seconds of inactivity, only those will be captured."]}),"\n"]})]}),"\n",(0,i.jsx)(n.h4,{id:"conversation-related-options-in-nodes",children:"Conversation related options in nodes"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Fields"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Enable acknowledgement message"})}),(0,i.jsx)(n.td,{children:"When this is enabled, an acknowledgement kind message (\u201chmmm\u201d OR \u201cokay\u201d) could be spoken in the conversation immediately. This is a small custom feature built to bring more human touch to the conversation."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Acknowledgement message"})}),(0,i.jsx)(n.td,{children:'Enter a text/SSML message depending upon the configuration under the Text Type field. Note - Keep it short for a better user experience.  Ex: "Do you want to confirm?"'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Boost phrases"})}),(0,i.jsx)(n.td,{children:"Some user responses can be confusing for the AI-agent to understand. Region specific words, new genz lingos, internet terminologies, trending phrases, abbreviations are trained specially so that the AI-agent understands the exact intention. For example, COVID is a new term that has been used frequently, the phrase COVID must be boosted, otherwise it gets translated to kovind/ go we/ co-wid etc.  Ex - you should add the phrases that you expect from the user response like, < I want to take covid vaccine >"})]})]})]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["Click ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/cookbooks/voice-as-channel/voiceoverview",children:"here"})," for voice related user guides."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"33-configure-node-for-google-assistant",children:"3.3 Configure node for Google assistant"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"End Session"})," - AI-agent session on google assistant comes to an end when this node is reached."]}),"\n"]}),"\n",(0,i.jsx)("img",{src:"https://i.imgur.com/FeCAyTA.png",alt:"drawing",width:"60%"}),"\n",(0,i.jsx)(n.h3,{id:"34-configure-node-for-alexa",children:"3.4 Configure node for Alexa"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"End Session"})," - AI-agent session on Alexa comes to an end when this node is reached."]}),"\n"]}),"\n",(0,i.jsx)("img",{src:"https://i.imgur.com/hE9OoiF.png",alt:"drawing",width:"60%"}),"\n",(0,i.jsx)(n.admonition,{type:"note",children:(0,i.jsxs)(n.p,{children:["The options will be displayed for each of the configured ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/channelConfiguration/overview",children:"channel"}),". Viber, Telegram, Whatsapp, Google assistant, Alexa, etc."]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.h2,{id:"-4-view-dynamic-data",children:[(0,i.jsx)("a",{name:"dynamic"})," 4. View dynamic data"]}),"\n",(0,i.jsxs)(n.p,{children:["Instead of adding information to these nodes manually, you can add objects to the Dynamic data. This option is available on most of the Prompt and Message nodes.\nYou can click on the ",(0,i.jsx)(n.code,{children:"i"})," next to ",(0,i.jsx)(n.strong,{children:"Fetch from"})," and find the variable data."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://i.imgur.com/DonapjB.png",alt:""})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What Next?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Build a ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/build/Flows/journeys",children:"flow"})," and ",(0,i.jsx)(n.a,{href:"https://docs.yellow.ai/docs/platform_concepts/studio/tools",children:"test"})," it."]}),"\n",(0,i.jsx)(n.li,{children:"Explore more about the types of nodes in the further sections."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);